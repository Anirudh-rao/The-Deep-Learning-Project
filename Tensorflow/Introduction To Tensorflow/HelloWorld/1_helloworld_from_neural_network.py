# -*- coding: utf-8 -*-
"""1.HelloWorld From Neural Network

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lsa5z50dkybkHO0gSFo7KNpf-SnQMpAS

# Hello World From Neural Network

We will create a simple Neural Network that takes the value of **x** and predicts **y**.

Lets build the same neural network in tensorflow and keras.
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras

print(tf.__version__)

model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.summary()

"""In the above we are creating a variable named **model**. To this variable we are defining the following:

1. Since we are bulding a nerual network we need to define neurons . This can be done using the **Dense** function provided by **Keras**. This function takes in `Input Shape` as an input.

2. The Next step is to stack the layer so that it forms the network. We can do this by using the function **Layers**. As we see in further examples, this function can vary from a single **1D Layer function ** to much more complex **2D Layer** function.

3. Once we complete the Layer  and Dense Part it complete one segmennt of the Neural Network. In order to have multiple layers and Dense units we need to stack it in a Sequential Layer, that is provided by the **Sequential** API of Keras.


"""

model.compile(optimizer='sgd',loss='mean_squared_error')

""" Now that we a `Single Neuron Neural Network` , this completes on **Forward Pass** in a Neural Network. The Forward Pass is a way for a machine learning Algorithm to learn. But How will it know weather its right or Wrong. This is achived by a **Back Prop** or **Back Propogation**. This is done using a loss function and an Optmizer. Here we will be using  [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) Optmizer and an [Mean Squared Error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError) loss function. This estimates the predicted values and in the backend changes the weights and bias's.

 Now lets start with our Inputs **X** and training data **Y**.
"""

Xs = np.array([-1.0,0.0,1.0,2.0,3.0,4.0], dtype='float')
ys = np.array([-3.0,-1.0,1.0,3.0,5.0,7.0], dtype='float')

"""Lets feed this into our neural network model. Here we are feeding the Training data **Xs** and corresponding labels **ys** . We also need to make sure that the neural network does not overfit or underfit , hence we use a term called **epcohs** this randomly splits the datasets into chunks and feed it to the neural network in steps."""

model.fit(Xs, ys, epochs=500)

"""Here we can observe that after each step the loss reduces. This in end goal to our Neural Network. To predict the input values effectivley.

To Test our model we will give some random input value to the neural Network.
"""

print(model.predict([10.0]))

"""Here the actual Value should be **19** but the neural network is giving us a value close to 19 . This due to the fact the the  
1. The Data that we fed into the neural network is small and very linear i.e the dataset is not diverse enough to predict the values.

2. This in turn gives a neural network an unsure question , hence it give the probable value and not the actual value as it itself is not sure.
"""

