{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this Example we will deep dive further Machine Learning and Deep learning. We will build what is called a **Computer Vision** task. Through this we can classifiy and predict images  that we feed as input.\n",
        "\n"
      ],
      "metadata": {
        "id": "EyH7JifwV1Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3ac4OgCW5M0",
        "outputId": "fb354ed8-df4d-4960-e865-7b99ad16e66d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this notebook we will use [Fasshion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This Dataset contains over **75 Thousand** examples , of which **60 Thousand** is Training dataset and rest **15 Thousand** is test set. The Images are all of `28x28` dimensions with 10 classes. Lets get the data."
      ],
      "metadata": {
        "id": "cE5NmYeIXFzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashionMnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images,train_labels), (test_images, test_labels) = fashionMnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQgCWscNXGMD",
        "outputId": "007e25c5-d04e-4bd7-8c1c-e4107d4769df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "index = 8\n",
        "\n",
        "#Set number of charecter per row when printing\n",
        "np.set_printoptions(linewidth=320)\n",
        "\n",
        "#print label\n",
        "print(f'LABEL:{train_labels[index]}')\n",
        "print(f'\\nIMAGE PIXEL ARRAY:\\n {train_images[index]}')\n",
        "\n",
        "plt.imshow(train_images[index], cmap='Greys')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "qIYQCok0b7U2",
        "outputId": "3e576cfe-9814-403d-c4b1-3ff059514d83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL:5\n",
            "\n",
            "IMAGE PIXEL ARRAY:\n",
            " [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   3   1   0   0   1   1   0   0   0   0  58   0  39   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   3   0   0   0   0   0   0   0  64 109 146 192 193   7   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0  94  38  99 209 183 229 192 142  48   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   0   0   0   0  41  45 158 146 164 114  51   1  53 105  42  36   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  10  68  44  30  59 172 146   0  22   0  13 103 111 103   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   3   1   0  22  61  88 152 255  71   0   0   0   0  35  85 112 201  44   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   0   0  13  62 154  62   0   0   0   0   0   0  54  99  61 106  51  19]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9   1   0   0   1   0  79  82  47  33  58  50]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   3   1   3   9   3   0   0   1   0 100  88  48  35  70  54]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   1   0   0 111 195 119  29  58  45]\n",
            " [  0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   3   3   0   0  91 146 171  16  93  35]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  45   3  79  87  99   6]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   0   0 119 137  33  96  77  13  45   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  32 160 164 142 116  79  82  39  39   0]\n",
            " [  0   0   0   0   3   0   0   0   0   0   0   0   0   3   4  10   0  41 180 142 171   1   0   0  48  73  16   0]\n",
            " [  0   0   0   0   0   0   1   1   0   0   0   0   0   0   3   0  27 155 114 169   0   0   0   0  47  76   6   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0 155 129 160   0   0   0   0   0  45  96   0   0]\n",
            " [  0   0   0   0   1   0  16  39  64   0   0   0   0   0   0 129 151 175   0   0   0   4   4   0  48 116   0   0]\n",
            " [  0   0   0   0   0   0  58  87  73  10   0   0   0   0  27 187 195   0   0   0   0   3   1   0  47 146   0   0]\n",
            " [  1   0   0   0   1   0   0   0   0   0   0   0   0   0 181 225  45   0   0   0   0   0   1   0  45 186   0   0]\n",
            " [  0   0   0   0   0   0   1 183 210  90   0   0   0 126 253 142   0   0   0   0   0   0   1   0  48 203   0   0]\n",
            " [ 64  58  45  27  16   9   1 175 245 204  22   0  70 236 190   6   0   0   0   0   0   0   0   0  50 196   0   0]\n",
            " [ 96 128 149 163 158 140 138 146 154 108  90 148 193 177  36   0   7   0   0   0   0   0   0   0  41 125   0   0]\n",
            " [  0   0   0   0  19  47  65  93  94 125 166 180 119  29   0   0   0   0   0   0   0   0   0   0  32 238   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 131   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79cd6dbae5f0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see these images are uniform , that is there values vary differntly for each image. So in order to have a uniform dataset, we will **normalize** the entire dataset from the value `0 to 255`."
      ],
      "metadata": {
        "id": "Yi6hyIYrfYPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images/255.0\n",
        "test_images = test_images/255.0"
      ],
      "metadata": {
        "id": "88V7fiWSfq68"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without further due lets build our neural network."
      ],
      "metadata": {
        "id": "ryhQ-PieX_0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)),\n",
        "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-ZNpc9zYHg8",
        "outputId": "4fb9a4b0-fb29-4c46-e3db-66c3366d1503"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101770 (397.54 KB)\n",
            "Trainable params: 101770 (397.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for this example , there is a lot changes from previous examples.\n",
        "1. Since our source data is an image of size `28x28` . But our neural network does not know how to read image data. So we will convert a `3x3x3` [This is due to the fact that images are compirsed of Layers of red blue and green]style image into `1x784`.This can be achieved using **Flatten** function from keras.\n",
        "This will output a matrix of size 1x784 as shown above.\n",
        "\n",
        "2. Once we have done this, we will do our usual **Dense** layer to compress the values from `1x784` to `1x128`. This can done using a **Activation Function**. An Activation function takes the output from neuron and regualirezs it from a value of `0 to 1`. Here we are using two activation functions : **relu** and **softmax** . These two are the most commonly use activation functions.\n",
        "\n",
        "3. The Output of the last layer is `1x10` and contains value from 1-9. This will be used to predict."
      ],
      "metadata": {
        "id": "hc_L4tvnYbpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9MCaKqMga0B3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are using a\n",
        "1. [Adam optmizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
        "2. [Sparse categorical cross entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)"
      ],
      "metadata": {
        "id": "gkVaApSBgblh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERAEA4Oygxl3",
        "outputId": "7772ac2e-b2ae-40f2-c80d-169360225977"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4982 - accuracy: 0.8258\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3788 - accuracy: 0.8639\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3367 - accuracy: 0.8767\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3137 - accuracy: 0.8846\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2956 - accuracy: 0.8906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79cd6da2e7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will evaluate our image using the test labels."
      ],
      "metadata": {
        "id": "0E4EkbC3g94W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD8fjV2IhBoT",
        "outputId": "81bd37dd-7741-4db8-a585-d8e824cc04ef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3528 - accuracy: 0.8730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3527677059173584, 0.8730000257492065]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also change the epochs so that the epochs change until the set efficiency is met. This is done using Callbacks()."
      ],
      "metadata": {
        "id": "TOaljRNKh1oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    '''\n",
        "    Halts the training after reaching 60 percent accuracy\n",
        "\n",
        "    Args:\n",
        "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
        "      logs (dict) - metric results from the training epoch\n",
        "    '''\n",
        "\n",
        "    # Check accuracy\n",
        "    if(logs.get('loss') < 0.2):\n",
        "\n",
        "      # Stop if threshold is met\n",
        "      print(\"\\nLoss is lower than 0.4 so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# Instantiate class\n",
        "callbacks = myCallback()"
      ],
      "metadata": {
        "id": "AdxnveDKh9kZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with a callback\n",
        "model.fit(train_images, train_labels, epochs=10, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT_yaDjIhr91",
        "outputId": "77ea36b1-716f-489f-bde5-24310d5d1eea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2683 - accuracy: 0.8999\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2572 - accuracy: 0.9047\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2482 - accuracy: 0.9065\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2396 - accuracy: 0.9105\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2330 - accuracy: 0.9130\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2255 - accuracy: 0.9148\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2181 - accuracy: 0.9186\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2120 - accuracy: 0.9208\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2053 - accuracy: 0.9235\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2010 - accuracy: 0.9244\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79cd6da2cbb0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gERbP0-i3FB",
        "outputId": "61b1f287-4fdd-476b-953b-915b3d2c7977"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3442 - accuracy: 0.8904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3441849946975708, 0.8903999924659729]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}