# -*- coding: utf-8 -*-
"""FashionMnist Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19I_-JcZ2YZk1rTqnanxqOowIY9dGCz65

In this Example we will deep dive further Machine Learning and Deep learning. We will build what is called a **Computer Vision** task. Through this we can classifiy and predict images  that we feed as input.
"""

import tensorflow as tf
import numpy as np
from tensorflow import keras
print(tf.__version__)

"""For this notebook we will use [Fasshion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This Dataset contains over **75 Thousand** examples , of which **60 Thousand** is Training dataset and rest **15 Thousand** is test set. The Images are all of `28x28` dimensions with 10 classes. Lets get the data."""

fashionMnist = tf.keras.datasets.fashion_mnist
(train_images,train_labels), (test_images, test_labels) = fashionMnist.load_data()

import matplotlib.pyplot as plt


index = 8

#Set number of charecter per row when printing
np.set_printoptions(linewidth=320)

#print label
print(f'LABEL:{train_labels[index]}')
print(f'\nIMAGE PIXEL ARRAY:\n {train_images[index]}')

plt.imshow(train_images[index], cmap='Greys')

"""As we can see these images are uniform , that is there values vary differntly for each image. So in order to have a uniform dataset, we will **normalize** the entire dataset from the value `0 to 255`."""

train_images = train_images/255.0
test_images = test_images/255.0

"""Without further due lets build our neural network."""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(128, activation=tf.nn.relu),
    keras.layers.Dense(10,activation=tf.nn.softmax)
])

model.summary()

"""Now for this example , there is a lot changes from previous examples.
1. Since our source data is an image of size `28x28` . But our neural network does not know how to read image data. So we will convert a `3x3x3` [This is due to the fact that images are compirsed of Layers of red blue and green]style image into `1x784`.This can be achieved using **Flatten** function from keras.
This will output a matrix of size 1x784 as shown above.

2. Once we have done this, we will do our usual **Dense** layer to compress the values from `1x784` to `1x128`. This can done using a **Activation Function**. An Activation function takes the output from neuron and regualirezs it from a value of `0 to 1`. Here we are using two activation functions : **relu** and **softmax** . These two are the most commonly use activation functions.

3. The Output of the last layer is `1x10` and contains value from 1-9. This will be used to predict.
"""

model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""Here we are using a
1. [Adam optmizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)
2. [Sparse categorical cross entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)
"""

model.fit(train_images, train_labels, epochs=5)

"""We will evaluate our image using the test labels."""

model.evaluate(test_images, test_labels)

"""We can also change the epochs so that the epochs change until the set efficiency is met. This is done using Callbacks()."""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    '''
    Halts the training after reaching 60 percent accuracy

    Args:
      epoch (integer) - index of epoch (required but unused in the function definition below)
      logs (dict) - metric results from the training epoch
    '''

    # Check accuracy
    if(logs.get('loss') < 0.2):

      # Stop if threshold is met
      print("\nLoss is lower than 0.4 so cancelling training!")
      self.model.stop_training = True

# Instantiate class
callbacks = myCallback()

# Train the model with a callback
model.fit(train_images, train_labels, epochs=10, callbacks=[callbacks])

model.evaluate(test_images, test_labels)